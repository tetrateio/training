# Improving Resiliency

Istio's traffic management features can be leveraged to both improve resiliency and prove out resiliency policies. In this section we will use fault injection to intentionally disrupt communication between services and then apply policies to remediate any user impact.

Istio makes fault injection simple because it’s decoupled from your application code. This means you can break things without making any changes to application code. The istio-proxy is intercepting all network traffic and can adjust responses and response speed. You can easily inject a variety of faults, including HTTP error codes (L7 faults) and network failures or delays (L4 faults).

## Fault Injection

### Aborts

Connect to the Hipstershop application, and select one item in the shop, like the Terrarium.

![Product view](/assets/Hipster_Shop-ads.png)

This page contains a detailed view of the product you selected, a button to buy the product then 4 other items that the shop propose, and a text advertisment.

We're going to inject an HTTP fault to simulate a transient failure in your `adservice` service, causing the ads to fail to load. To do this, you’ll add a fault stanza to the account `VirtualService` causing an HTTP 500 response code for 50% of calls to the accounts service. 

![Abort](/assets/hipstershop-istio-ingress-abort.svg)

This stanza configures Envoy to return a 500 response code immediately when a client calls the account service instead of forwarding the request to the service itself.

```yaml
kubectl apply -n hipstershopv1v2 -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: adservice
spec:
  hosts:
  - adservice.hipstershopv1v2.svc.cluster.local
  http:
  - route:
    - destination:
        host: adservice.hipstershopv1v2.svc.cluster.local
    fault:
      abort:
        httpStatus: 500
        percentage: 
          value: 50
EOF
```

Now if you refresh the Hipstershop page a couple of times, sometimes you won’t see any adds because we received a 500 from the Istio-proxy.

![Product view noadds](/assets/Hipster_Shop-noads.png)

> Note that the error 500 is NOT coming from the `adservice` itself. I you look at the logs of the `adservice`, you will only see good GET requests with code 200. The error is generated by the `Istio-proxy` started along the "client" (the `frontend` service in this demo). If you query the `adservice` from a pod which is NOT using Istio, you will get 100% success.

To ensure users aren't impacted by these request failures we have a couple tools in our toolbox: retries and outlier detection.

### Delays

Like `abort`, the `delay` is a way to introduce unsusual behaviour inside the mesh by adding a delay before sending the request to the upstream. Like `aborts`, they are enforced on the client side. 

![Abort](/assets/hipstershop-istio-ingress-delay.svg)

Let's add a 4s delay to the request:

```yaml
kubectl apply -n hipstershopv1v2 -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: adservice
spec:
  hosts:
  - adservice.hipstershopv1v2.svc.cluster.local
  http:
  - route:
    - destination:
        host: adservice.hipstershopv1v2.svc.cluster.local
    fault:
      delay:
        percentage:
          value: 100
        fixedDelay: 4s
EOF
```
Let's do some requests on the frontend.
If we look at the frontend's logs, here is what we see:
```yaml
GET /ad HTTP/1.1" 200 DI "-" "-" 0 113 4002 2
```

first, the request succeeded: we have a code 200, and a flag `DI`. According to the doc: `DI: The request processing was delayed for a period specified via fault injection.`
Then the last two numbers show the request duration:

- the `4002` number is the request duration as seen by the `frontend` service. The request took 4 seconds, which match the delay we defined in the VirtualService
- the last numner, `2` is the request duration to the upstream cluster (the `adservice` service). It took only 2ms.

We clearly see that we waited for 4s before calling the upstream.


### Retries

Retries refer to the act of retrying a failed HTTP request to guard against transient failures. They can be used for any safe (GET) or idempotent (PUT/DELETE) requests.

![Abort](/assets/hipstershop-istio-ingress-retry.svg)

> There is currently a [bug](https://github.com/istio/istio/issues/13705) in Istio where if you set fault injection AND retries then the retries do not take effect. If you set only one or the other then they work.

To be able to demonstrate this scenario, we are going to use a feature from the `adservice` microservice. 
For that, remove the `VirtualService` we just created. We also scale the `adservice-v2` microservice to 0:

```shell
kubectl -n hipstershopv1v2 delete vs adservice
kubectl -n hipstershopv1v2  scale --replicas=0 deployment adservice-v2
kubectl -n hipstershopv1v2  scale --replicas=0 deployment frontend-v2
```

Then edit the `adservice` deployment and add the `CONSECUTIVEERROR=2` environment variable so 2 requests out of 3 will return an error. 
We are also adding a small lattency of 2 seconds, to better demonstrate what's going on during a retry.

```shell
kubectl -n hipstershopv1v2 patch deployment adservice --type='json' -p='[{"op": "add", "path": "/spec/template/spec/containers/0/env", value: [{"name":"CONSECUTIVEERROR","value":"2"},{"name":"EXTRA_LATENCY","value":"2s"},{"name":"LOGLEVEL","value":"debug"},{"name":"SRVURL","value":":9555"}]}]'
```

The default behaviour of Istio is to retry two times, so we are going to turn it down to 0 so we can experience a failure:

Let’s turn off retries.

```yaml
kubectl apply -n hipstershopv1v2 -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: adservice
spec:
  hosts:
  - adservice.hipstershopv1v2.svc.cluster.local
  http:
  - route:
    - destination:
        host: adservice.hipstershopv1v2.svc.cluster.local
    retries:
      attempts: 0
EOF
```

As before, 50% of the requests have the `advertisements` block at the bottom of the page.
If we look at the access logs of the frontend, we clearly see that one out of three requests to the `adservice` service is having a 500 error code:

```yaml
  "GET /ad HTTP/1.1" 200 - "-" "-" 0 123 2031 2030 "-" "Go-http-client/1.1" "09f236b7-dd59-4ae2-8a67-11a49417da31" "adservice.hipstershopv1v2:9555" "10.56.2.27:9555" outbound|9555||adservice.hipstershopv1v2.svc.cluster.local 10.56.0.48:40966 10.122.15.107:9555 10.56.0.48:51888 - -
  "GET /ad HTTP/1.1" 503 - "-" "-" 0 52 2003 2003 "-" "Go-http-client/1.1" "fddbbb61-5289-4d6f-8d18-a885245f1992" "adservice.hipstershopv1v2:9555" "10.56.2.27:9555" outbound|9555||adservice.hipstershopv1v2.svc.cluster.local 10.56.0.48:40966 10.122.15.107:9555 10.56.0.48:51888 - -
```

To be precise, let's look at the start of the access logs. Remember the formating: `<METHOD> <PATH> <PROTOCOL> <RESPONSE_CODE> <RESPONSE_FLAG> "x" "x" <BYTES_RECEIVED> <BYTES_SENT>` then the more interresting part here: `<REQ_DURATION> <UPSTREAM_DURATION>`

We see both requests beeing arount 2000ms of response-time. This is on par with the configuration we made, the 2s delay.

Let's add 2 retries on errors. We also add `retryOn` configuration to add, beside default cases, the `5xx`, which will retry on any `500, 501, 502...` errors. We need this as a `500` error is not retriable by default:

```yaml
kubectl apply -n hipstershopv1v2 -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: adservice
spec:
  hosts:
  - adservice.hipstershopv1v2.svc.cluster.local
  http:
  - route:
    - destination:
        host: adservice.hipstershopv1v2.svc.cluster.local
    retries:
      attempts: 2
      perTryTimeout: 5s
      retryOn: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes,5xx
EOF
```

After few reloads on the Hipstershop frontend, we always have the `Advertisement` block. Our problem is solved !

If we look again at the logs, truncated to the interresting part:

```yaml
 "GET /ad HTTP/1.1" 200 - "-" "-" 0 113 2030 2029 "
 "GET /ad HTTP/1.1" 200 - "-" "-" 0 137 6074 6074 "
```

Here, the first request took `2030ms`. We were lucky and the first call to the `adservice` was successful.

The second request took `6074ms` (6 seconds) to succeed... Let's look at the `adservice` logs:

```yaml
 "GET /ad HTTP/1.1" 503 - "-" "-" 0 52 2001 2000 
 "GET /ad HTTP/1.1" 503 - "-" "-" 0 52 2001 2001 
 "GET /ad HTTP/1.1" 200 - "-" "-" 0 123 2001 2000 
```
Here, from the `adservice` perspective, 3 requests came in and the first two failed. Each request took `2000ms`, which correspond to the overall 6s as seen by the `frontend` service.

### Timeouts & Retries

Networks are unreliable. Consequently, transient request latency is a common occurance in a distributed system. So how can we protect our users from this? Well, we can combine retries and timeouts. Rather than wait the 2s for the request to complete, we will timeout after 1s and then retry, repeating until one of our requests succeed or we fail 3 consecutive times. Let’s apply that configuration to the microservice that calls the `adservice` service, the frontend service.

Let's recall what we have so far: the `adservice` is answering with a 2s delay, and 2 requests out of 3 are 503 errors. We added a retry policy, so we do 2 more attempts if we get a 5xx error.

In the logs that means:

```yaml
"GET /ad HTTP/1.1" 503 - "-" "-" 0 52 2001 2000 "-" "Go-http-client/1.1" "e6bcb163-1776-404d-a228-77ad9fec0f09" "adservice.hipstershopv1v2:9555" ...
"GET /ad HTTP/1.1" 503 - "-" "-" 0 52 2001 2000 "-" "Go-http-client/1.1" "e6bcb163-1776-404d-a228-77ad9fec0f09" "adservice.hipstershopv1v2:9555" ...
"GET /ad HTTP/1.1" 200 - "-" "-" 0 123 2001 2000 "-" "Go-http-client/1.1" "e6bcb163-1776-404d-a228-77ad9fec0f09" "adservice.hipstershopv1v2:9555" ...
```
first 2 are 503 errors, then a 200. All these requests share the same `RequestID`, `e6bcb163-1776-404d-a228-77ad9fec0f09`, explicitelly showing us that they are retried.

Now, add an overal 6s timeout and a 1s timeout per retry. Note that the 6s timeout is a global timeout for the requests. The request will *never* be longer than 6s. If we configure 7 retries with a 1s delay, we will never retry 7 times, as the global 6s timeout will kick-in before the exhaust the number of retries. Here's our test yaml:

```yaml
kubectl apply -n hipstershopv1v2 -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: adservice
spec:
  hosts:
  - adservice.hipstershopv1v2.svc.cluster.local
  http:
  - route:
    - destination:
        host: adservice.hipstershopv1v2.svc.cluster.local
    timeout: 6s
    retries:
      attempts: 2
      perTryTimeout: 1s
      retryOn: connect-failure,refused-stream,unavailable,cancelled,retriable-status-codes,5xx
EOF
```

If you do a single requests, you get in the `frontend` logs:

```yaml
"GET /ad HTTP/1.1" 504 UT,URX "-" "-" 0 24 3015 -
```

UT means `Upstream request timeout in addition to 504 response code.`, which also explain the 504 error code.
URX means `The request was rejected because the upstream retry limit (HTTP) or maximum connect attempts (TCP) was reached.`, which says that the first try plus the 2 retries failed. You can also see this from the two last part of the log: request took 3015 ms (3 times 1s timeout) and there was no upstream duration, as we never waited for it, we closed the connection after 1s.

![Abort](/assets/hipstershop-istio-ingress-timeout1.svg)

Looking at the `adservice` logs:

```yaml
"GET /ad HTTP/1.1" 0 DC "-" "-" 0 0 1000 - "-" "Go-http-client/1.1" "0a082d3e-c389-40e8-b467-870a1d71a0da"
"GET /ad HTTP/1.1" 0 DC "-" "-" 0 0 1000 - "-" "Go-http-client/1.1" "0a082d3e-c389-40e8-b467-870a1d71a0da"
"GET /ad HTTP/1.1" 0 DC "-" "-" 0 0 999  - "-" "Go-http-client/1.1" "0a082d3e-c389-40e8-b467-870a1d71a0da"
```

DC stands for `Downstream connection termination.` which state that the `frontend` service (the caller, or client, or downstream in Istio's language) closed the connection after 1s.

By adding the Timouts, we went from a 100% service response with a 6s delay to a 100% error with 3 seconds. This demo is not going to help us to solve our production problems...

To be closer to reallity, let's start the `adservice-v2` service. It's the same as the current one, but without delay or errors. In this situation, Istio (Envoy) should load-balance requests between the two services and keeps retrying when an error arise.

```shell
kubectl -n hipstershopv1v2  scale --replicas=1 deployment adservice-v2
```

![Abort](/assets/hipstershop-istio-ingress-timeout2.svg)

After doing some requests, you should see in the frontend logs that some requests are answered right away and some others are answered, but with a 1s delay:

```rtf
"GET /ad HTTP/1.1" 200 - "-" "-" 0 113 85 85
"GET /ad HTTP/1.1" 200 - "-" "-" 0 121 1019 1019
```

On the `adservice` side, we either see one request to `adservice-v2` or failed request to `adservice` AND another on the `adservice-v2` with the same RequestID.
```rtf
adservice: "GET /ad HTTP/1.1" 0 DC "-" "-" 0 0 999 - "-" "Go-http-client/1.1" "32553f81-69e6-4456-ab08-da38fb6e2638"
adservice-v2: "GET /ad HTTP/1.1" 200 - "-" "-" 0 97 1 0 "-" "Go-http-client/1.1" "32553f81-69e6-4456-ab08-da38fb6e2638"
```

By setting Request Timeouts, we went from a 6s successful answer to a 1s answer. This demo does not reflect a real production scenario. In real life, you will have many instances of the same `adservice`, and some of them, at some point, will start giving slow answers, some in 1s, some in 10s or up. Request Timeouts and retries are more effective when all apps does not answer at the same slowliness. But I hope you get the idea.

#### Homework

find a way to demonstrate, for the same `adservice` application, timeouts under 1s, 2s and 3s. What will happen then ?

answer: ducplicate deployment `adservice` as `adservce-v3` and set a 3s timeout. you will have a chance to experiment 2 timeouts in a row before getting a valid answer from the `adservice-v2` deployment


### Outlier Detection

Outlier detection and ejection, a form of passive health checking, is the act of determining if some of the endpoints to which we're sending traffic are performing differently than the others and avoiding sending traffic to the outliers. We say that the endpoints we avoid have been "ejected from the active load balancing set." For any given service, Envoy always maintains a set of healthy endpoints for that service: the active load balancing set.

Typically, endpoints are ejected from the active load balancing set based on consecutive error responses. With HTTP services this would be consecutive 5xx failures. With TCP services, connect timeouts and connection errors/failures would lead to ejection. Over time, Envoy will attempt to add ejected endpoints back into the active load balancing set by sending traffic to them. If the endpoint responds successfully, it's reintroduced to the active load balancing set. Otherwise, it remains in the ejected set.

Outlier detection and retries are used in conjunction to improve resiliency. Outlier detection will increase success rate by ejecting endpoints that are deemed unhealthy, and retries mask any failures to users calling our application. 

First, let's re-start the v2 version of the `adservice` if not already there:

```shell
kubectl -n hipstershopv1v2  scale --replicas=1 deployment adservice-v2
```

If you do some requests on the frontend and check the `adservice` and `adservice-v2` logs you will see both queried. You will also notice that, as before, `adservice` is giving 2 503 errors out of 3 requests. We strongly suggest to install `stern` if you havne't done so yet. Use `stern adservice` to see logs of both pods at the same time.
You can use the following `curl` command to do one query per second:

```shell
while true ; do curl -s  -o /dev/null http://hipstershop.35.245.245.42.sslip.io/ ; sleep 1s ; done
```

![Abort](/assets/hipstershop-istio-ingress-outlier1.svg)

We can now add outlier detection by creating a `DestinationRule` (more on those in later sections). An example of how we would configure outlier detection is below. In it, a service instance will be ejected if it returns a 5xx on 2 consecutive attempts. It will only be allowed back in after 5 minutes multiplied by the number of times it has been ejected.

```yaml
kubectl apply -n hipstershopv1v2 -f - <<EOF
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: adservice
spec:
  host: adservice.hipstershopv1v2.svc.cluster.local
  trafficPolicy:
   outlierDetection:
      consecutiveErrors: 2
      baseEjectionTime: 5m
EOF
```

It won't be long before you only see requests going to `adservice-v2`.

![Abort](/assets/hipstershop-istio-ingress-outlier2.svg)

By using an *Outlier-Detection* rule we were able to guaranty a near-perfect quality of service.

Do you remember how we used the *Traffic Routing* features of Istio to load-balance traffic between the `v1` and `v2` versions of the `frontend` application ? The traffic split happen before we send the request to the destination, so if no `v2` frontend pods are deployed, we will still send requests to it and get errors.
This is exactly where the Outlier Detection can come in play. By updating the `DestinationRule` of the `frontend` application, we can ensure that no traffic will be sent to the `v2` service if no pods can answer. 
To be even more precise, after two consecutives errors (two requests trying to reach the `v2` that will fail), we will stop sending requests to the `v2` application. That's a 2 fail per 5 minutes:

```yaml
kubectl apply -n hipstershopv1v2 -f - <<EOF
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: frontend-subset
  namespace: hipstershopv1v2
spec:
  host: frontend.hipstershopv1v2.svc.cluster.local
  subsets:
  - labels:
      version: v1
    name: v1
  - labels:
      version: v2
    name: v2
  trafficPolicy:
    outlierDetection:
      baseEjectionTime: 5m
      consecutiveErrors: 2
EOF
```

## Clean-up

```shell
kubectl -n hipstershopv1v2  delete dr adservice
kubectl -n hipstershopv1v2  delete vs adservice
```

## Takeaway

In this module we demonstrated how to use Istio Service Mesh bricks to improve reliability of our service without touching at our application's code.

---
Next step: [Security](/modules/security)